\documentclass[a4paper,10pt,oneside]{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{subfigure}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tgpagella}
%\usepackage[scale=0.9]{tgcursor}
%\usepackage[scale=0.9]{tgheros}
\usepackage{xstring}

\newcommand{\myscale}{0.74}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\code}[1]{\texttt{\StrSubstitute{#1}{.}{.\.}}}
\def\.{\discretionary{}{}{}}
\newcommand{\jmodule}[1]{\texttt{\textit{#1}}}

\setlength{\hoffset}{-1in} %left margin will be 0, as hoffset is by default 1inch
\setlength{\voffset}{-1in} %analogous voffset
\setlength{\oddsidemargin}{1.5cm}
\setlength{\evensidemargin}{1.5cm}
\setlength{\topmargin}{1.5cm}
\setlength{\textheight}{24cm}
\setlength{\textwidth}{18cm}

\def\mftitle{jInfer TwoStepSimplifier design and implementation}
\def\mfauthor{Michal Klempa, Mário Mikula, Robert Smetana, Michal Švirec, Matej Vitásek}
\def\mfadvisor{RNDr. Irena Mlýnková, Ph.D., Martin Nečaský, Ph.D.}
\def\mfplacedate{Praha, 2011}
\title{\bf\mftitle}
\author{\mfauthor \\ Advisors: \mfadvisor}
\date{\mfplacedate}

\ifx\pdfoutput\undefined\relax\else\pdfinfo{ /Title (\mftitle) /Author (\mfauthor) /Creator (PDFLaTeX) } \fi

\begin{document}
\maketitle
\noindent Target audience: developers willing to extend jInfer, scientist willing to implement own inference methods

\noindent \emph{Note: we use the term \textbf{inference} for the act of creation of schema throughout this and other jInfer documents.}

\noindent \begin{tabular}{|l|l|} \hline
Responsible developer: & Michal Klempa \\ \hline
Required tokens:       & cz.cuni.mff.ksi.jinfer.base.interfaces.RuleDisplayer \\ \hline
Provided tokens:       & cz.cuni.mff.ksi.jinfer.base.interfaces.inference.Simplifier \\ \hline
Module dependencies:   & AutoEditor, Base, JUNG, Lookup \\ \hline
Public packages:       & \\ \hline
\end{tabular}

\section{Introduction}
With \jmodule{TwoStepSimplifier} we provide user complex solution of infering problem.
This is the only one simplifier shipped with jInfer, but it has many replaceable submodules of which we implement more alternatives in basic bundle.
The best we implement is \cite{ahonen} $k,h$-context method od infering grammar rules.
This process is split into submodules with generic automaton implementation, so it is easy to plug-in any other automaton merging state algorithm.
In this document, we will walk over all modules and submodules in simplifier and describe them briefly.

\section{Using of factory pattern}
But in first, we will stay a while at factory pattern usage.
In \jmodule{TwoStepSimplifier} we use factory pattern (as described in \cite{archdoc}) to divide module into submodules.
As service providing classes are kept singleton, we make them factories:
\begin{verbatim}
public interface AutomatonSimplifierFactory extends 
                     NamedModule, Capabilities, UserModuleDescription {
  <T> AutomatonSimplifier<T> create();
}
\end{verbatim}
The real submodule interface (called familiarly worker interface) is the \code{AutomatonSimplifier<T>} interface (of wich instance is returned by factory). It is now not important how worker interface looks like, lets examine the factory.

In \code{TwoStepSimplifier} we design service providing factory interfaces so that they extends extends \code{NamedModule} with module describing methods:
\begin{verbatim}
String getName();
String getDisplayName();
String getModuleDescription();
\end{verbatim}
And to extend \code{Capabilities}, thus each module have to answer, if it has some capabilities.
And to extend \code{UserModuleDescription} interface, which defines method:
\begin{verbatim}
String getUserModuleDescription();
\end{verbatim}
It returns description of the module, comprehensive to user, which is then displayed in properties panels.

In some circumstances, it is useful to have method \code{create()} generic.
The \jmodule{AutomatonSimplifier} works with \code{Automaton}, which itself is generic too.
But simplifying does not depend on type of symbol of automaton, so interface \code{AutomatonSimplifier<T>}
is also generic as simplifier can simplify automaton of any java type symbol.
Factory interface deals with this by defining \code{create()} method generic too.

Our usage of this factory pattern follows the routine:
\begin{verbatim}
final Properties p = RunningProject.getActiveProjectProps(getName());

AutomatonSimplifierFactory f = ModuleSelectionHelper.lookupImpl(AutomatonSimplifierFactory.class,
   p.getProperty(PROPERTIES_AUTOMATON_SIMPLIFIER));

AutomatonSimplifier<AbstractStructuralNode> autSmp = f.<AbstractStructuralNode>create();
...
result= autSmp.work(something_to_process);
\end{verbatim}
If one module has some submodules, we implement lookups for submodule implementations in our own factory \code{create} method.
Worker class receives factories of all of its submodules as a constructor parameters.

Lets look on \code{AutomatonMergingStateFactory}, that is factory of module, which has \jmodule{AutomatonSimplifier} as submodule.
Its create method looks like this (shortened): 
\begin{verbatim}
@Override
public ClusterProcessor<AbstractStructuralNode> create() {
  LOG.debug("Creating new ClusterProcessorAutomatonMergingState.");
  return new AutomatonMergingState(getAutomatonSimplifierFactory(), getRegexpAutomatonSimplifierFactory());
}
\end{verbatim}
Methods \code{getAutomatonSimplifierFactory} and \code{getRegexpAutomatonSimplifierFactory} are analogical, we show you the former one:
\begin{verbatim}
private AutomatonSimplifierFactory getAutomatonSimplifierFactory() {
  final Properties p = RunningProject.getActiveProjectProps(getName());

  return ModuleSelectionHelper.lookupImpl(AutomatonSimplifierFactory.class,
            p.getProperty(PROPERTIES_AUTOMATON_SIMPLIFIER));
}
\end{verbatim}
Cluster processor \code{AutomatonMergingState} then receives factories of \code{AutomatonSimplifier} and \code{RegexpAutomatonSimplifier} submodules in its constructor.
Cluster processor then may create as many instances of submodule classes as it needs (maybe simplifying more than one automaton).
Thorough this document, we will mention only worker interface when describing submodules, since all factory interfaces are designed same way just described.

\section{Structure}
\jmodule{TwoStepSimplfier} is implemented in package \code{cz.cuni.mff.ksi.jinfer.twostep}.
Module is implemented in two classes: \code{TwoStepSimplifier} (main logic), \code{TwoStepSimplifierFactory} (lookups, interface to other modules).
Latter one is registered as service provider (and implements) of \code{Simplifier} interface (defined in package \code{cz.cuni.mff.ksi.jinfer.base.interfaces.inference}).

It has main method called \code{start} which receives Initial Grammar in form of
\begin{verbatim}
final List<Element> grammar
\end{verbatim}
Each grammar rule is represented as class \code{Element}, where element is left side of rule and it's \code{getSubnodes()} method
returns regular expression, which is right side of rule.
Second parameter is 
\begin{verbatim}
final SimplifierCallback callback
\end{verbatim}
Callback which should be called when work is done.
On output, simplifier provides simplified grammar as a parameter into callback function:
\begin{verbatim}
void finished(final List<Element> grammar);
\end{verbatim}
Thus simplifier receives initial grammar and returns simplified grammar, both represented same way - as elements.

\code{TwoStepSimplifierFactory} in method \code{start()} creates new \code{TwoStepSimplifier} class instance, providing three factories of submodules to its constructor.
Then it calls \code{simplify(initialGrammar)} method on this instance and its result passes as parameter to callback function.
Thats all, the magic comes in \code{TwoStepSimplifier} class.

\jmodule{TwoStepSimplifier} is inspired by \cite{1802522} design. Inference proceeds in two steps:
\begin{enumerate}
	\item clustering of element instances into clusters of (probably) same elements
	\item infering regular expression for each element from examples of element contents taken from all elements in cluster
\end{enumerate}
Task of clustering is dedicated to \jmodule{Clusterer} submodule, and task of infering regular expression for each cluster is dedicated to \jmodule{ClusterProcessor} submodule. We will examine both of them.
There is also third submodule called \jmodule{RegularExpressionCleaner} actually, its purpose is just to beatify output regular expressions, no inference logic is implemented there. 
Modules are drawn on fig. \ref{twostep_modules}.
\begin{figure}
	\centering\includegraphics[scale=\myscale]{twostep_modules}
	\caption{Submodules of TwoStep simplifier with their implementations} \label{twostep_modules}
\end{figure}
We provide one \jmodule{Clusterer} and 4 \jmodule{ClusterProcessor} implementations.
Each of those will be explained further in this document.

Method \code{TwoStepSimplifier.simplify()} does basically this:
\begin{verbatim}
// 1. cluster elements
final Clusterer<AbstractStructuralNode> clusterer= clustererFactory.create();
clusterer.addAll(initialGrammar);
clusterer.cluster();

// 2. prepare emtpy final grammar
final List<Element> finalGrammar= new LinkedList<Element>();

// 3. process rules
final ClusterProcessor<AbstractStructuralNode> processor =
             clusterProcessorFactory.create();
             
for (Cluster<AbstractStructuralNode> cluster : clusterer.getClusters()) {
  final AbstractStructuralNode node =  
                          processor.processCluster(clusterer, cluster.getMembers());
  final RegularExpressionCleaner<AbstractStructuralNode> cleaner = 
                          regularExpressionCleanerFactory.<AbstractStructuralNode>create();
  // 4. add to rules
  finalGrammar.add(
              new Element(node.getContext(),
              node.getName(),
              node.getMetadata(),
              cleaner.cleanRegularExpression(((Element) node).getSubnodes()),
              attList));
}

return finalGrammar;
\end{verbatim}
It creates clusterer, gives it all rules, orders it to cluster elements.
Then, empty list of elements is created as final (simplified) grammar.
For each input rule in initialGrammar, submodule \jmodule{ClusterProcessor} is called to do infering of regular expression of that element.
Finally, regular expression cleaning is done in submodule and new \code{Element} instance is created as a copy of processed node, but with cleaned regexp.

Now we will examine submodules for clustering, processing and cleaning.

TODO anti sentinel cant be on left side
TODO anti sentinel processing

\subsection{Clusterer module}
Lets start by example, let input document(s) contain XML:
\begin{verbatim}
<person name="john" surname="smith">
  <info>
    Some text
    <note/>
  </info>
</person>
<inform>
  Another text
  <note/>
</inform>
<person>
    <information>
      Some text
      <note/>
    </information>
</person>
\end{verbatim}
We examine info-like named elements.
If we cluster elements sipmly by their name, we get one cluster with info element, another cluster with inform element and another one with information element.
As \cite{1802522} suggests, we should consider element content in clustering process.
If we take care of element content, elements info, inform and information would look same (in means of node tree inside them - text node and element note).
Elements information and info have even same context (inside person element).

We implement clustering in \code{cz.cuni.mff.ksi.jinfer.twostep.clustering} package.
One cluster is represented by class \code{Cluster<T>} with \code{T} as type of clustered items.
This class simply holds java set of member of cluster and one of the references is held also in \code{representant} member.

We provide \code{Clusterer} interface for classes to implement.
Its purpose is to cluster bunch of elements (rules) on input, into  bunch of cluster class instances (cluters) on output.
It has methods \code{add()} and \code{addAll()} for adding items for clustering.
Centerpart is method \code{cluster()}, which does the clustering itself.
As it may be time-consuming operation, method throws \code{InterruptedException}.
Implementation should take care of checking whether thread is user interrupted (see \cite[p. 12]{archdoc}).
After clustering, implementation should hold clusters in member, as it it will be further asked by calling method \code{T getRepresentantForItem(T item)}.
Given item to this method, one can ask for representant of cluster, to which the item belongs.
If no such cluster exists (item was not added for clustering before), we recommend you throwing an exception rather than returning \code{null}.
Missing item will probably  indicate error in algorithm rather than normal workflow.
One can pull clusters (\code{List<Cluster<T>{}>}) from clusterer by calling \code{getClusters()} method.

Basic work usage of clusterer is:
\begin{verbatim}
Clusterer <T> c = new MyContextClusterer<T>();
c.addAll(initialGrammar);
c.cluster();
...
c.getClusters();
or
c.getRepresentantForItem(x);
\end{verbatim}

\subsubsection{ClustererWithAttributes extended interface}
Maybe you noticed that whole clusterer interface and cluster class are generic.
They may be used as design pattern not only for clustering elements in inference process.
To address clustering of elements in more detail, we created \code{ClustererWithAttributes<T, S>} interface, which extends \code{Clusterer<T>} interface.
It adds method \code{List<Cluster<S>{}> getAttributeClusters(T representant)}, implying that each representant of
type \code{T} (that is representant of some main cluster) has some "attribute" clusters associated with it.
Attribute clusters are of type \code{S} and can be retrieved by calling \code{getAttributeClusters(x)}.

Finally, we use this scheme to implement clusterer \code{Iname<AbstractStructuralNode, Attribute>} class, which takes \code{AbstractStructuralNode} classes to cluster as main, and \code{Attribute} classes as attributes.
Clustering is done based on elements \code{getName} equality test (ignore case).
For each rule (element), it is first clustered by finding cluster where representant has same name - or create new cluster with this element as representant.
Then we process right side of this element rule (that are nodes from \code{getSubnodes}).
Since right side of rule is always concatenation, we simply take \code{.getSubnodes().getTokens()} list and iterate through it.
Each node on right side, is examined:
\begin{itemize}
  \item if it is simple data throw it to \code{SimpleDataClusterer} class which is associated with main element (of which this is right side), which does nothing more, than holds all of simple data in one cluster. But in future it may be replaced to cluster simple data somehow, to obtain meaningful content models in schemas.
  \item if it is element and it is tagged as sentinel by metada - search main clusters to find cluster with representant of same \code{getName()}, or create new cluster with this sentinel as representant. From IGG, sentinels may be only on right sides of rules and since each element in schemas has to be defined, there must exist another element with same name, which is not sentinel (and maybe it will come to process in future). So there can't be cluster with only one sentinel element in it.
  \item do nothing otherwise, since it is element, that has to have its subnodes defined, and therefore it is element that is proper grammar rule and therefore it has to be somewhere in main initial grammar list, thus it is already processed or will be on schedule in clustering.
\end{itemize}
Attributes of element are processed through helper attribute clusterer, which is created for each element cluster.
We have bunch of elements of same name in cluster and one attribute clusterer associated with this bunch.
This attribute clusterer is given all attributes instances encountered in all elements that are in bunch.
It clusters them by name (case insensitive).

To rehearse, there are main clusters for elements, for each main cluster there are two helper clusterer - one for attribute clusters and one for simple data clusters.

\subsection{ClusterProcessor module}
Cluster processor takes rules of one cluster of element and somehow obtains regular expression for that set of elements. It returns one rule - element with name set to desired name of element in schema (not all elements in cluster have to have same name, if advanced clustering scheme is used, then processor has to choose right name for the resulting element) and with subnodes set to regular expression infered.
It process attributes of all elements in cluster to obtain meaningful schema attribute specification and theese attributes has to attach to resulting element.

Worker interface itself is defined as follows:
\begin{verbatim}
public interface ClusterProcessor<T> {
  T processCluster(
          final Clusterer<T> clusterer,
          final List<T> rules
          ) throws InterruptedException;
}
\end{verbatim}
Maybe you are asking, why cluster processor is given the clusterer instance.
Rules themself contain information about which elements to process, but clusterer has more information
about the topic.
Clusterer can tell you representant for any element in whole input (not only those elements in rules, but also those that may be on right side of rules), clusterer (if it is with attributes) has information about attributes <of each cluster.

We will now shortly describe each cluster processor implementation we've got.

\subsubsection{PassRepresentant cluster processor}
Simple example to read. For each cluster return its representant as a rule to be in schema.
This has nothing to do with infering grammar, it is just proof of submodules concept.
Input documents are not valid against this odd grammar.
Do not use this in practice, just read the code to understand the bare minimum needed to implement submodule.

\subsubsection{Alternations cluster processor}
This processor simply gets all right sides from elements in cluster, puts them in one big list and creates alternation regular expression with this list as children.
That is, it creates one big rule with alternation of every positive example observed.
No generalization is done at all.

\subsubsection{Trie cluster processor}
This processor takes all rules in a cluster, treats them like strings and builds a prefix tree (a ``trie") of them. More precisely, it takes the first rule and delares it to be a long branch (concatenation of tokens) in a newly created tree. After that, it adds the remaining rules one by one as branches like this: as long as it can follow an existing branch, it follows it. As soon as the newly added branch starts to differ, it ``branches off" (creates an alternation at that point) the existing tree and hangs the rest of the newly added rule there. Repeating this process creates a prefix tree describing all the rules in the cluster.
TODO vektor and then what? how do you get regexp from that.

\subsubsection{AutomatonMergingState cluster processor}
\begin{figure}
	\centering\includegraphics[scale=\myscale]{automaton_merging_state_modules}
	\caption{Submodules of AutomatonMergingState cluster processor} \label{automaton_merging_state_modules}
\end{figure}
\jmodule{AutomatonMergingState} is implementation of merging state algorithm on nondeterministic finite automaton (see package \code{cz.cuni.mff.ksi.jinfer.twostep.processing.automatonmergingstate}).
It creates prefix-tree automaton (PTA) from positive examples - right sides of rules given.
Then it calls its submodule called \jmodule{AutomatonSimplifier} to  modify PTA to some generalized 
automaton by merging states.

Simplified automaton is then converted to an instance of \code{RegexpAutomaton} by using clone constructor.
Regexp automaton is automaton with regular expression as symbol on transitions.
In automata theory, such automaton is called extended NFA.
Clone constructing is done by converting each symbol in source automaton to regexp token with that symbol as \code{content}.

Regexp automaton is then passed into second submodule called \jmodule{RegexpAutomatonSimplifier}.
Its job is to derive regular expression from automaton, such that automaton and regular expression represents same language.

\jmodule{AutomatonMergingState} has one more submodule, the \jmodule{MergeConditionTester}, which is not called directly by \jmodule{AutomatonMergingState}.
It is at disposal for implementations of \code{AutomatonSimplifier} interface for testing, whether two states in automaton are equivalent and should be merged into one state.

Module \jmodule{AutomatonSimplifier} implements solution searching logic in simplifying automaton by merging states.
One can implement ACO heuristics or MDL principle heuristics as \jmodule{AutomatonSimplifier} submodule, and can use any of \jmodule{MergeConditionTesters} provided.

Shortened code of cluster processor operation:
\begin{verbatim}
// 1. Construct PTA
final Automaton<AbstractStructuralNode> automaton = new Automaton<AbstractStructuralNode>(true);

// Take each rule in cluster and pass right side to automaton to create PTA
for (AbstractStructuralNode instance : rules) {
  final Element element = (Element) instance;
  final Regexp<AbstractStructuralNode> rightSide = element.getSubnodes();

  final List<AbstractStructuralNode> rightSideTokens = rightSide.getTokens();

  final List<AbstractStructuralNode> symbolString = new LinkedList<AbstractStructuralNode>();
  for (AbstractStructuralNode token : rightSideTokens) {
    symbolString.add(clusterer.getRepresentantForItem(token));
  }
  automaton.buildPTAOnSymbol(symbolString);
}

// 2. Simplify automaton by merging states using automatonSimplifier
final Automaton<AbstractStructuralNode> simplifiedAutomaton =
    automatonSimplifier.simplify(automaton, elementSymbolToString);

// 3. Convert Automaton<AbstractStructuralNode> to RegexpAutomaton<AbstractStructuralNode>
final RegexpAutomaton<AbstractStructuralNode> regexpAutomaton =
    new RegexpAutomaton<AbstractStructuralNode>(simplifiedAutomaton);
// 4. Call regexpAutomatonSimplifier to obtain regular expression from regexpAutomaton
final Regexp<AbstractStructuralNode> regexp =
    regexpAutomatonSimplifier.simplify(regexpAutomaton, regexpAbstractToString);

// 5. Return element with regexp
return new Element(
      new ArrayList<String>(),
      rules.get(0).getName(),
      new HashMap<String, Object>(),
      regexp,
      new ArrayList<Attribute>()
      );
\end{verbatim}
Whole submodule structure of \jmodule{AutomatonMergingState} cluster processor is drawn on fig. \ref{automaton_merging_state_modules}.

\subsubsection{AutomatonSimplifier submodule}
\jmodule{AutomatonSimplifier} module has its worker interface defined as:
\begin{verbatim}
public interface AutomatonSimplifier<T> {
  Automaton<T> simplify(final Automaton<T> inputAutomaton, 
                        final SymbolToString<T> symbolToString) throws InterruptedException;
}
\end{verbatim}
Given input automaton it returns automaton, there is nothing magical on it.
Interface is defined as generic, implementation don't have to be, but there is no reason to not make them generic.
When implementation needs to present automaton to user, however, it needs some string representation of symbols to display on automaton transitions.
For this reason, the second parameter, \code{symbolToString} (implementation of \code{SymbolToString} interaface) is given.
It is responsible for converting symbol to string.
To \jmodule{AutomatonSimplifier} it is passed from \jmodule{AutomatonMergingState} cluster processor, where we know that symbols are of type \code{AbstractStructuralNode}, so we implement \code{SymbolToString} by using \code{getName()} method on nodes and pass it down to generic \code{AutomatonSimplfifier} submodule.

By this genericity, simplifier can be used to perfom same algorithms for simplifying not only to infer rules for elements, but maybe also to infer patterns strings for content model of attributes.
The actual implementation of this still waits for its developer to come.

TODO how automaton works?

Lets go on to implementations of \jmodule{AutomatonSimplifier}.

\paragraph{Greedy automaton simplifier} We implement greedy strategy of automaton simplifying in class \code{Greedy}.
It simply asks given \code{MergeConditionTester} if it can merge any of automaton states and merges states until there are no states to be merged:
\begin{verbatim}
while (there exist pair of states that are equivalent) {
  merge them
}
\end{verbatim}
We have implemented \emph{k,h-context} (see \cite{ahonen}) state equivalence in class \code{KHContext} (implements \code{MergeConditionTester} interface), which is used by \code{Greedy} to test mergability of states by default configuration.

\paragraph{UserInteractive automaton simplifier}
We have created automaton simplifier which displays user input automaton asking him to select some states to merge.
Then it merges them and asks user again:
\begin{verbatim}
repeat {
  draw automaton
  if (user selects more than one state to merge) {
    merge them
  }
} until (user selected at least one state in last trial)
\end{verbatim}
We user \jmodule{AutoEditor} module described in \cite{autoeditordoc}.
To make reader more happy, we include one screenshot of automaton visualization on picture \ref{screenshot_viz}.
TODO anti screenshot for happy reader

\subsubsection{RegexpAutomatonSimplifier submodule}
Purpose of \jmodule{RegexpAutomatonSimplifier} is to obtain regular expression from given automaton.
Interface for doing this is thus simple:
\begin{verbatim}
public interface RegexpAutomatonSimplifier<T> {
  Regexp<T> simplify(final RegexpAutomaton<T> inputAutomaton, 
                     final SymbolToString<Regexp<T>> symbolToString) throws InterruptedException;
}
\end{verbatim}
Once again we are encountering \code{symbolToString} with same purpose as before, if submodule would need to present automaton to user
it has to be able to convert symbols of (at runtime) unknown type to strings.

\begin{figure}
	\centering\includegraphics[scale=\myscale]{state_removal_modules}
	\caption{Modules of StateRemoval.} \label{state_removal_modules}
\end{figure}
\paragraph{StateRemoval regexp automaton simplifier}
We are using state removal method (see \cite{1224321}) to convert regexp automaton into equivalent regular expression.
This is implemented in \code{StateRemoval} class (on fig. \ref{state_removal_modules}) which implements \code{RegexpAutomatonSimplifier} interface.
State removal works by removing states of automaton (and redirecting transitions properly) until there are last to two states - superInitial and superFinal.
Both are added before algorithm starts. Former one with $\lambda$-transition to initial state, and from all final states $\lambda$ transition to superFinal state is added.
After removing all states, there is only one transition from superInitial to superFinal state.
That transition has final regular expression on it as symbol, it is read and returned to \jmodule{AutomatonMergingState}.

We defined one submodule of this class with \jmodule{Orderer}.
It has only one method to implement: \code{getStateToRemove}.
Given automaton, it has to return reference to one state which should be removed from automaton at first.
State removal calls this submodule and removes state returned.

\paragraph{Weighted} We implement one orderer, called \code{Weighted}.
It is simple heuristic - weights all states (weight = sum of {in | out | loop}-transition regular expression lengths) and returns state with lowest weight.

\subsection{RegularExpressionCleaner module}
Last we examine \code{RegularExpressionCleaner} interface.
Purpose of this submodule is only to make output regular expression corrections, to make them nicer.
For example it is common that converting automaton to regular expression by state elimination produces nested concatenations such like $(name, (person, id))$.
To convert such expression into $(name, person, id)$, one can implement this interface and connect it to work in chain.
Interface is definition is straightforward:
\begin{verbatim}
public interface RegularExpressionCleaner<T> {
  Regexp<T> cleanRegularExpression(final Regexp<T> regexp);
}
\end{verbatim}
Given regular expression, return regular expression.
Converting of regular expression is commonly a recursive task and all our implementations work this way.
Lets look at them.

\paragraph{NestedConcatention}
This cleaner does exactly what we have just described.
It converts nested concatenations into flat ones.

\paragraph{Null}
This cleaner just returns regexp it receives and does nothing.
Plug this one into chain, if you want to omit this step of inference.

\paragraph{EmptyChildren}
Wipes out regular expressions of type: concatenation, alternation, permutation, which have empty \code{children} member.
For example some pitty inferring method produces unnecessary empty regexps in regexp tree as:
$$
(name, (), (person, id))
$$
There is one empty concatenation/alternation/permutation with no sense.
This is wiped out by this cleaner.

\paragraph{Chained}
Enables user to chain more cleaners with output from first one plugged as input into second one and so one.
Thus regular expression from inference can pass through \jmodule{EmptyChildren} and them through \jmodule{NestedConcatenation} cleaners and then it is returned.


Whole TwoStep submodules structure is on fig. \ref{twostep_modules_complete}.
\begin{figure}
	\centering\includegraphics[scale=\myscale]{twostep_modules_complete}
	\caption{Modules of TwoStep simplifier and their submodules. Filled classes are default selection (best of).} \label{twostep_modules_complete}
\end{figure}

%\nocite{*}
\newpage
\bibliographystyle{alpha}
\bibliography{literature}

\end{document}
